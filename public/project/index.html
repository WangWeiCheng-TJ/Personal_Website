<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: January 9, 2026 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.3.1" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Wang Wei-Cheng" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/project/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/emerald.min.css" />
  

  
  
    
    <link href="/dist/wc.min.40d365a5c94bd94585e708f7c92e5782e00a8d8eefc348f5d2f21a80bb7783c8.css" rel="stylesheet" />
  

  
  
  

  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu3247630877640252165.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu4166356570829923896.png" />

  <link rel="canonical" href="http://localhost:1313/project/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Wei-Cheng Wang&#39;s Personal Website" />
  <meta property="og:url" content="http://localhost:1313/project/" />
  <meta property="og:title" content="Projects | Wei-Cheng Wang&#39;s Personal Website" />
  <meta property="og:description" content="" /><meta property="og:image" content="http://localhost:1313/media/icon_hu7729264130191091259.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_hu7729264130191091259.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2026-01-07T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2026-01-07T00:00:00&#43;00:00">
  

  



  


  <title>Projects | Wei-Cheng Wang&#39;s Personal Website</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  







<link type="text/css" rel="stylesheet" href="/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css" integrity="sha256-vnZutBkxehTsdp0hbpd5v&#43;jzc3yA54D0ug2vtXpBpII=" />


<script src="/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js" integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script>


<script>window.hbb.pagefind = {"baseUrl":"/"};</script>

<style>
  html.dark {
    --pagefind-ui-primary: #eeeeee;
    --pagefind-ui-text: #eeeeee;
    --pagefind-ui-background: #152028;
    --pagefind-ui-border: #152028;
    --pagefind-ui-tag: #152028;
  }
</style>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({
      element: "#search",
      showSubResults: true,
      baseUrl: window.hbb.pagefind.baseUrl,
      bundlePath: window.hbb.pagefind.baseUrl + "pagefind/",
    });
  });
  document.addEventListener('DOMContentLoaded', () => {
    let element = document.getElementById('search');
    let trigger = document.getElementById('search_toggle');

    if (trigger) {
      trigger.addEventListener('click', () => {
        element.classList.toggle('hidden');
        element.querySelector("input").value = ""
        element.querySelector("input").focus()

        if (!element.classList.contains('hidden')) {
          let clear_trigger = document.querySelector('.pagefind-ui__search-clear');

          if (clear_trigger && !clear_trigger.hasAttribute('listenerOnClick')) {
            clear_trigger.setAttribute('listenerOnClick', 'true');

            clear_trigger.addEventListener('click', () => {
              element.classList.toggle('hidden');
            });
          }
        }

      });
    }
  });
</script>















  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>



<link rel="stylesheet" href="/css/custom.css" type="text/css">
  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Wei-Cheng Wang&#39;s Personal Website">
            
            
              
              <img
                fetchpriority="high"
                decoding="async"
                class=""
                width="206"
                height="36"
                src="/media/Elf_white_hu7399649101897331904.webp"
                alt="Wei-Cheng Wang&#39;s Personal Website" />
            
          
        
        
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >Bio</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#papers"
        >Papers</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/post/mission/"
        >Research Philosophy</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/experience/"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link active"
          
          href="/project/"
        >Projects</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      
      <button
        aria-label="search"
        class="text-black hover:text-primary  inline-block px-3 text-xl dark:text-white"
        id="search_toggle">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512" fill="currentColor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352a144 144 0 1 0 0-288 144 144 0 1 0 0 288z"/></svg>
      </button>
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>


<div id="search" class="hidden p-3"></div>


        
      
    </div>
    <div class="page-body ">
      

  
  
    








  

  

  

  

  






























  
    
    
    
  





























<section id="section-collection" class="relative hbb-section blox-collection  " style="padding: 5rem 0 5rem 0;" >
 <div class="home-section-bg " >
   
 </div>
  

  

    











  









  
  
  
  
  






















  





<div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0">

  <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
    Applied AI Systems
  </div>

  <p><div style="text-align: justify;">
While my Ph.D. trained me to dissect complex problems with academic rigor, my drive for human connection, rooted in my community leadership, inspires me to build tangible solutions.
<p>Since graduating, I bridge these worlds by leveraging Large Language Models (LLMs) and modern AI stacks to rapidly transform ideas into deployed systems. The following projects represent my agile approach to AI engineering: identifying critical daily needs and engineering practical, high-utility solutions that make life safer and easier.</p>
</div></p>
</div>


<div class="flex flex-col items-center px-6">

  
  


<div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-[500px] justify-center">
  <div class="grid gap-10 md:grid-cols-1 lg:gap-10">


  
    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project-applied/ugentapp/" >

      <img alt="Taiwanese at Ghent Survivor Kit - A Serverless LLM-Agent Deployment"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project-applied/ugentapp/featured_hu1638343288362908652.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/llm-agent/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">LLM Agent</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project-applied/ugentapp/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Taiwanese at Ghent Survivor Kit - A Serverless LLM-Agent Deployment
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project-applied/ugentapp/" >
        TW@Ghent Survivor Kit : AI-Powered Community Platform
A serverless, AI-driven information hub designed to automate community management and solve information fragmentation for international students.
Motivation &amp; Product Philosophy TW@Ghent Survivor Kit is a comprehensive survival guide platform. Originally engineered for students, I collaborated with the current president of UGent Taiwanese Student Association (TSA) to redefine the product roadmap, expanding its scope to serve the entire Taiwanese expatriate community. This ensured the system aligned with actual operational needs rather than just technical novelty.
“I built this not just as a developer, but as the former President who identified the root cause of platform failure.”
I recognized that previous platforms failed due to high operational friction. To solve this, I set a strict constraint: The system must be “low-maintenance” and operable by non-tech staff. This drove the decision to adopt a serverless architecture combined with autonomous AI agents, allowing for rapid iteration and a “set-and-forget” operational model.
Role: Product Owner &amp; Full-Stack Engineer Scope: Requirement Analysis → System Architecture → AI Agent Development → CI/CD Tech Stack AI &amp; NLP: Python, Gemma 3 4B (LLM), Feedparser (RSS), Prompt Engineering Backend / CMS: Google Sheets API (NoSQL/CMS), Google Apps Script, Event-Driven ETL Frontend: Next.js 14 (App Router), TypeScript, Tailwind CSS, ISR Infrastructure: Vercel (Serverless), GitHub Actions (CI/CD), Docker Technical Architecture &amp; Implementation 1. AI-Driven Intelligence Pipeline (Event-Driven ETL) The core innovation is an automated pipeline that monitors, analyzes, and translates local news without human intervention, effectively functioning as a domain-specific AI agent:
Data Ingestion: A Python-based agent continuously monitors municipal RSS feeds (stad.gent) and emergency alerts. LLM Integration (Gemma 3 4B): Deployed Gemma 3 4B to perform semantic analysis on raw Dutch texts. Structured Prompt Engineering: Designed rigorous prompt templates to enforce valid JSON output from the LLM. Tasks include: Importance Grading (Level 1-3), Audience Classification (Student vs. Resident), Traditional Chinese Translation, and Summarization. Robustness: Implemented retry logic with exponential backoff to handle API rate limits and ensure pipeline reliability. ETL Execution: Structured data is automatically validated and written back to the Google Sheets CMS, triggering frontend updates. 2. Serverless Full-Stack Architecture Designed a cost-efficient architecture suitable for long-term operation:
Headless CMS (Google Sheets): Abstracted Google Sheets into a JSON API. This allows non-technical staff to manage content via a familiar spreadsheet interface, eliminating database costs ($0/month) and lowering the maintenance barrier. Frontend (Next.js 14): Implemented incremental static regeneration (60s revalidation) to ensure high performance and SEO while keeping data fresh. 3. CI/CD &amp; DevOps GitHub Actions: Orchestrated daily cron jobs (UTC 6:00) to execute the news crawling and AI analysis agents. Security &amp; Reproducibility: Managed API Secrets via GitHub Secrets and utilized docker to ensure environment consistency for the AI agents. Automated Deployment: Configured Vercel for automatic deployments on git push, establishing a production-ready lifecycle. Key Results &amp; Impact 100% Automation: Achieved a fully automated loop for news gathering, translation, classification, and publishing. Zero Operational Cost: Leveraged serverless tiers to maintain costfree_, ensuring the project’s financial sustainability for the student association. Solved “Technical Debt”: Created a system that requires no coding skills to maintain, addressing the high turnover rate inherent in student organizations. Resources Live Website GitHub Repository AI Agent Source Code - Python agent for scraping and LLM processing. Prompt Engineering Templates - Structured prompts for Gemma 3-4B. </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2026-01-06">Jan 6, 2026</time>
        </div>
      </div>

    </div>
  </div>
</div>

  

    </div>
</div>


</div>






  

  
</section>

  

  
  
    








  

  

  

  

  






























  
    
    
    
  





























<section id="section-collection" class="relative hbb-section blox-collection  " style="padding: 5rem 0 5rem 0;" >
 <div class="home-section-bg " >
   
 </div>
  

  

    











  









  
  
  
  
  






















  





<div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0">

  <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
    Research Projects
  </div>

  <p><div style="text-align: justify;">
While my applied work prioritizes user-centric utility, it is built upon a foundation of rigorous academic inquiry established during my doctoral and master's studies. <br>
My academic journey at Ghent University and NCKU centered on analyzing real-world data within the fields of surveillance and driver monitoring, with a specific emphasis on audio-visual modalities. I investigated the critical gap between controlled lab environments and unpredictable real-world deployments, proposing novel mechanisms and unsupervised frameworks to bridge this divide. My research spans computer vision, audio processing, and multimodal representation learning, extending into privacy preservation and transferability assessment. This body of work represents my dedication to pushing the boundaries of what AI can perceive without compromising the rights of the people it protects.
</div>
</p>
</div>


<div class="flex flex-col items-center px-6">

  
  


<div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-screen-lg ">
  <div class="grid gap-10 md:grid-cols-3 lg:gap-10">


  
    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project-research/phd/" >

      <img alt="Transferable and Privacy-friendly Deep Learning Techniques for Audio-Visual Urban Surveillance - From Lab to Street"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project-research/phd/featured_hu11025979831286906752.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/unsupervised-learning/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Unsupervised Learning</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project-research/phd/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Transferable and Privacy-friendly Deep Learning Techniques for Audio-Visual Urban Surveillance - From Lab to Street
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project-research/phd/" >
        Bridging the gap between academic models and real-world deployment.
My Ph.D. research tackles the critical bottlenecks that prevent deep learning models from moving “from lab to street”: privacy risks, data scarcity, and environmental domain shifts. I proposed a unified framework for urban surveillance that balances performance with ethical compliance and robustness.
The dissertation is built upon three core technical pillars:
Privacy-Friendly Visual Analysis: Addressing the trade-off between utility and privacy. I developed source-free and label-free obfuscation techniques that allow models to detect anomalies while stripping away sensitive biometric information, ensuring compliance with strict regulations like GDPR.
Audio-Visual Representation Learning: Overcoming the reliance on expensive manual labels. I introduced contrastive learning frameworks that leverage the natural synchronization of sight and sound to learn robust feature representations in a self-supervised manner.
Transferability Assessment: Ensuring reliability in changing environments. I designed metrics to assess model transferability across different camera views and domains without requiring access to source data, enabling safer and more predictable deployment in the wild.
(This research culminated in a Ph.D. degree from Ghent University and 5 publications in top-tier journals/conferences.)
(Details to be followed)</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2025-06-30">Jun 30, 2025</time>
        </div>
      </div>

    </div>
  </div>
</div>

  
    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project-research/sensecity/" >

      <img alt="SensCity - Acoustic Surveillance in Real-World"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project-research/sensecity/featured_hu5399673042188975350.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/urban-acoustic-surveillance/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Urban Acoustic Surveillance</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project-research/sensecity/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">SensCity - Acoustic Surveillance in Real-World
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project-research/sensecity/" >
        SensCity x AsaSense: Critical Analysis of Urban Acoustic Surveillance
A strategic research collaboration with the SensCity project (AsaSense), utilizing city-scale raw acoustic data to expose the failure modes of standard surveillance models and proposing context-aware architectural solutions.
The Research Gap &amp; Motivation Why “Off-the-Shelf” Fails in the Wild:
Most acoustic surveillance systems are validated on clean, curated datasets. However, their performance on raw, unprocessed urban audio remains largely unverified.
Our Mission:
In collaboration with AsaSense, we accessed a unique stream of continuous, uncurated audio from Ghent and Rotterdam. Instead of just deploying a standard model, our goal was to stress-test two dominant paradigms: anomaly detection and sound tagging, and identify why conventional paradigms fail in dynamic environments (e.g., temporal drift, open-set events), and propose robust alternatives.
Operational Context (The SensCity Testbed) This project leveraged a real-world infrastructure to diagnose algorithmic limitations:
Raw Data Ingestion:
Unlike academic datasets, the SensCity sensor network captures the “messy” reality of cities across two years: wind noise, overlapping soundscapes, and non-stationary backgrounds. Most importantly, without any annotations. System Audit:
We applied SOTA approaches on anomaly detection and sound tagging models to this raw stream. The analysis revealed that global models generate unmanageable false alarms due to contextual blindness (e.g., treating a weekend market as an anomaly because the model only knew weekday traffic), further causing operator fatigue and leading to system failure. Core Conclusion:
Our experiments conclusively proved that a single global model is insufficient for city-scale deployment. Instead, Context-Specific Modeling (sensor-specific baselines) is a prerequisite for operational reliability. Proposed Resolution:
Based on these findings, we formulated a Context-Aware Design Framework, advocating for sensor-specific baselines and adaptive thresholding to handle the inherent variance of city life. Core Methodologies Data Source: High-fidelity, long-term raw acoustic logs from the AsaSense deployment (Ghent &amp; Rotterdam). Diagnosis Method: Cross-context evaluation (Spatial &amp; Temporal Domain Shift). Algorithmic Focus: Unsupervised Deep Autoregressive Modeling (WaveNet) vs. Pre-trained Tagging Models. Architecture Design: Feasibility analysis of Hybrid Edge-Cloud pipelines to mitigate bandwidth bottlenecks. Technical Analysis &amp; Innovations 1. Diagnosing the “Generalization Fallacy” The Problem: We demonstrated that state-of-the-art anomaly detectors suffer from severe concept drift. A model trained on “winter data” failed catastrophically during summer evenings due to changed human activity patterns. The Solution: Proposed a Context-Specific Modeling approach, proving that training lightweight, dedicated models for each sensor location significantly outperforms a massive, generic global model in anomaly retrieval. 2. The Limits of Semantic Tagging The Finding: Standard sound taggers (trained on AudioSet) struggle with the Open-Set Nature of cities. They force novel urban sounds into rigid, pre-defined categories, leading to semantic misalignment. The Proposal: Suggested moving from “rigid classification” to “unsupervised deviation detection” at the edge, using tagging only as a secondary enrichment layer in the cloud, rather than a primary filter. 3. Architectural Scalability (Edge vs. Cloud) Analysis: Analyzed the trade-off between transmission cost and detection latency. Recommendation: Proposed a “Filter-then-Forward” architecture where edge nodes perform lightweight unsupervised screening, transmitting only potential anomalies to the cloud. This reduces bandwidth consumption by orders of magnitude while preserving privacy. Outcomes &amp; Impact Empirical Evidence: Provided one of the first comprehensive studies on the limitations of transfer learning in acoustic surveillance using real-world, longitudinal data. Design Guidelines: The findings established the foundation for Privacy-Preserved &amp; Adaptive Surveillance, directly influencing the design of subsequent research on privacy in surveillance. Strategic Value: Delivered critical insights to the industrial partner (AsaSense) on avoiding “technical debt” by pivoting from global models to adaptive, edge-based learning. Resources Chapter 2: The AsaSense Project - Detailed analysis of deployment constraints and algorithmic failures. </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2021-06-30">Jun 30, 2021</time>
        </div>
      </div>

    </div>
  </div>
</div>

  
    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project-research/prephd/" >

      <img alt="Intelligent Video Analytics & Surveillance Systems"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project-research/prephd/featured_hu7377141018090171087.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/unsupervised-learning/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Unsupervised Learning</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project-research/prephd/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Intelligent Video Analytics &amp; Surveillance Systems
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project-research/prephd/" >
        Extracting insights from chaos without labeled data.
This research project focuses on the unsupervised understanding of surveillance video, tackling the full pipeline from raw pixel processing to user-centric visualization.
The core analysis module leverages background modeling to extract foreground entities, constructing trajectory kinematics descriptors to capture motion patterns. By applying unsupervised clustering on these spatiotemporal features, the system automatically distinguishes between normal routines and anomalous events without requiring manual annotations.
Beyond detection, my Master’s thesis addressed the challenge of information presentation. I formulated the dynamic annotation placement as a spatiotemporal optimization problem. By enforcing coherence constraints, the algorithm calculates optimal label positions that maximize readability while minimizing occlusion of critical visual information, ensuring a seamless monitoring experience.
(Details and visual results to be followed)</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2016-06-30">Jun 30, 2016</time>
        </div>
      </div>

    </div>
  </div>
</div>

  
    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project-research/drivermonitor/" >

      <img alt="Multimodal Driver Monitoring & Temporal Face Analysis"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project-research/drivermonitor/featured_hu7469397495544783819.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/driver-monitoring/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Driver Monitoring</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project-research/drivermonitor/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Multimodal Driver Monitoring &amp; Temporal Face Analysis
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project-research/drivermonitor/" >
         Multimodal Driver Safety System &amp; Robust Face Analysis
A holistic driver monitoring framework developed with ARTC, fusing visual temporal dynamics and ECG signals to enable early anomaly detection and proactive safety intervention.
The Research Gap &amp; Motivation From Passive Recording to Proactive Intervention:
Standard recognition models often fail in real-world cockpits due to inter-personal variability. A generic model struggles to distinguish between a driver’s natural features (e.g., droopy eyelids) and fatigue.
Our Goal: To build a safety-critical system capable of early detection of compromised states by combining non-intrusive visual monitoring with physiological signals (ECG), reducing false alarms and ensuring timely intervention.
Operational User Scenario (How it Works) To address the variability mentioned above, the system operates in a three-stage safety loop:
Initialization (The “Handshake”): When the driver starts the car, the system silently records a short “calibration sequence” to learn their current appearance (e.g., wearing sunglasses, heavy makeup, or fatigue). This establishes a Personalized Normal Driving Model (PNDM) for the specific trip. Dynamic Monitoring: As the vehicle moves through changing environments (e.g., entering a dark tunnel or facing high-beam glare), the alignment-free visual descriptor maintains robust tracking without being confused by lighting shifts. Proactive Intervention: If the driver shows signs of drowsiness (e.g., prolonged eye closure) AND the ECG sensor detects physiological fatigue, the system triggers a multi-stage alert—first warning the driver, and in critical cases, notifying fleet management or emergency services. Core Methodologies Visual Algorithms: Temporal Coherent Face Descriptor (alignment-free, robust to lighting). System Integration: Multimodal Sensor Fusion (Vision &#43; ECG). Modeling Strategy: Sparse Representation-based Classification with online dictionary learning. Validation: Co-developed and tested with the Automotive Research &amp; Testing Center (ARTC). Technical Architecture &amp; Innovations 1. Personalized Calibration (User-Centric Design) The Problem: Drivers look different every day. Pre-trained generic models fail when users change appearance.
The Solution: Implemented a rapid initialization phase that builds a dynamic baseline for each trip. The algorithm detects anomalies based on relative deviation from this baseline, effectively filtering out noise from accessories or facial structure.
2. Robust Temporal Modeling (Visual Subsystem) Alignment-Free: By leveraging temporal consistency across continuous frames, we eliminated the need for fragile face alignment steps, ensuring stability even under rapid head movements.
Lighting Invariance: Utilized intensity contrast descriptors to maintain accuracy in challenging lighting conditions (e.g., nighttime driving validated in NCKU-driver database).
3. Proactive Safety Trigger (System Level) Multimodal Logic: Designed the visual module to work in tandem with ECG sensors. While ECG detects physiological drops in alertness, our visual module confirms behavioral lapses (e.g., nodding off).
Impact: This cross-verification significantly reduces false positives, ensuring that alerts are only triggered for genuine safety risks.
Outcomes &amp; Validation Industry Collaboration: Co-developed with ARTC. Award-Winning: Secured Second Place at the International ICT Innovative Services Awards. Performance: Achieved real-time performance and superior accuracy over state-of-the-art baselines in nighttime scenarios. Resources Publications: Wang Wei-Cheng, Ru-Yun Hsu, Chun-Rong Huang, Li-You Syu (2015). Video gender recognition using temporal coherent face descriptor. IEEE/ACIS SNPD 2015.
Chien-Yu Chiou, Wang Wei-Cheng, Shueh-Chou Lu, Chun-Rong Huang, Pau-Choo Chung, Yun-Yang Lai (2019). Driver Monitoring Using Sparse Representation With Part-Based Temporal Face Descriptors. IEEE T-ITS. </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2016-06-30">Jun 30, 2016</time>
        </div>
      </div>

    </div>
  </div>
</div>

  

    </div>
</div>


</div>






  

  
</section>

  

  
  
    








  

  

  

  

  






























  
    
    
    
  





























<section id="section-collection" class="relative hbb-section blox-collection  " style="padding: 5rem 0 5rem 0;" >
 <div class="home-section-bg " >
   
 </div>
  

  

    











  









  
  
  
  
  






















  





<div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0">

  <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
    Research Engineering
  </div>

  <p><div style="text-align: justify;">
"Where theoretical rigor meets production constraints."
<p>This section showcases my work in translating complex research algorithms into robust, deployable systems. Here, the focus is on performance, reliability, and architectural precision.</p>
</div></p>
</div>


<div class="flex flex-col items-center px-6">

  
  


<div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-[500px] justify-center">
  <div class="grid gap-10 md:grid-cols-1 lg:gap-10">


  
    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project-tech/sysnopsis/" >

      <img alt="Bridging the gap between lab research and market value."
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project-tech/sysnopsis/featured_hu13426445731220845871.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/tech-transfer/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Tech Transfer</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project-tech/sysnopsis/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Bridging the gap between lab research and market value.
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project-tech/sysnopsis/" >
        I led the commercialization of the lab’s core research by developing a market-ready prototype and a viable business model. My role involved translating technical algorithms into a user-centric solution for surveillance efficiency.
This venture won the APICTA Merit Award (R&amp;D category) and secured funding, leading to a successful acquisition of the product/team by our industry partner, EverSTek. This experience honed my ability to identify user needs, build MVPs, and execute a successful technology exit.
(Details to be followed)</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2016-06-30">Jun 30, 2016</time>
        </div>
      </div>

    </div>
  </div>
</div>

  

    </div>
</div>


</div>






  

  
</section>

  


    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by text-center">
    © 2026 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
